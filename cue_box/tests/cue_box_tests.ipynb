{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a941fa0-01a4-40c5-89d4-ffdb00a7300a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U nutter chispa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7027a3c7-19b7-46df-95e0-d08806573c01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f848dc9-e50c-4ccf-be14-484f844a6fd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b383dd8-068a-402b-962e-cf299af19270",
     "showTitle": true,
     "title": "Common Imports, Functions & Schema"
    }
   },
   "outputs": [],
   "source": [
    "%run /Repos/potturi.tulasiram@diggibyte.com/cue-box/cue_box/utility/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34eedd5-d6e0-4bbc-8cc3-8d3500510037",
     "showTitle": true,
     "title": "Common Imports for Unit Testing"
    }
   },
   "outputs": [],
   "source": [
    "from runtime.nutterfixture import NutterFixture, tag\n",
    "from chispa.dataframe_comparer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc16c262-33e4-4350-a375-a5f7f7a09601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType([\n",
    "                        StructField('user_id', LongType(), True),\n",
    "                        StructField('name', StringType(),  True),\n",
    "                        StructField('email', StringType(),True),\n",
    "                        StructField('password', StringType(), True),\n",
    "                        StructField('phone', StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e0bc11-de54-4dd0-be64-df668fb8b54d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users = spark.read.format('csv').option('header','true').option('delimiter',',').schema(my_schema).load('/mnt/meta/data/unit_testing/users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c19d61-0636-4f31-903a-917d14053e49",
     "showTitle": true,
     "title": "Unit Test Cases Class"
    }
   },
   "outputs": [],
   "source": [
    "class TestFixtureArbitraryFiles(NutterFixture):\n",
    "  def __init__(self):\n",
    "    NutterFixture.__init__(self)\n",
    "\n",
    "  # Get custom Schema test case\n",
    "  def assertion_get_custom_schema(self):\n",
    "    df_expected = spark.createDataFrame(data =[],schema=my_schema)\n",
    "    expected_schema = get_custom_schema('users')\n",
    "    df_actual = spark.createDataFrame(data = [],schema=expected_schema)\n",
    "    assert_df_equality(df_actual, df_expected)\n",
    "\n",
    "  #Column value replace function test case\n",
    "  def assertion_replace_charactrs(self):\n",
    "    replacements = {\"password\":('@',\"\")}\n",
    "    \n",
    "    data = [\n",
    "      (9666488188,'Miss Beth Buchanan','Miss_Beth_Buchanan590@example.com','R_sCLwLzbW6c&I','469-349-07'),\n",
    "      (6986658652,'John Olson','John_Olson257@example.com','KX)FBXzP7Ue','606.814.97'),\n",
    "      (5870824764,'Gary Malone','Gary_Malone659@example.com','$7_HlF6g','001-902-76'),\n",
    "      (3463026079,'Kristin Sanders','Kristin_Sanders125@example.com','k#5nOh!g','+1-818-794')\n",
    "    ]\n",
    "    df_expected = spark.createDataFrame(data=data,schema=my_schema)\n",
    "    actual_value = replace_characters(users, replacements)\n",
    "    actual_df = actual_value.limit(4)\n",
    "    assert_df_equality(actual_df, df_expected)\n",
    "    \n",
    "  #Date format Change test case\n",
    "  def assertion_change_date_format(self):\n",
    "    expected_data = [(1, '01-01-2022', '2022-02-15'),(2, '30-03-2022', '2022-04-10'),(3, '20-05-2022', '2022-06-05')]\n",
    "    my_schema = StructType([\n",
    "                            StructField('id', StringType(), True),\n",
    "                            StructField('date_column_1', StringType(), True),\n",
    "                            StructField('date_column_2', StringType(), True)])\n",
    "    \n",
    "    input_data = [(1, '2022-01-01', '2022-02-15'),(2, '2022-03-30', '2022-04-10'),(3, '2022-05-20', '2022-06-05')]\n",
    "    \n",
    "    # Define schema with two date columns\n",
    "    \n",
    "    df_date = spark.createDataFrame(input_data,my_schema)\n",
    "    \n",
    "    df_expected = spark.createDataFrame(data=expected_data,schema=my_schema)\n",
    "    df_actual = change_date_format(df_date,['date_column_1'],'dd-MM-yyyy')\n",
    "    assert_df_equality(df_actual, df_expected)\n",
    "\n",
    "    # Column rename Function test cases\n",
    "  def assertion_rename_columns(self):\n",
    "      my_schema = StructType([\n",
    "                            StructField('id', LongType(), True),\n",
    "                            StructField('names', StringType(),True ),\n",
    "                            StructField('email', StringType(), True),\n",
    "                            StructField('password', StringType(), True),\n",
    "                            StructField('phone', StringType(), True)])\n",
    "      data = [\n",
    "        (9666488188, 'Miss Beth Buchanan', 'Miss_Beth_Buchanan590@example.com', 'R_sCLwLzbW6@c&I', '469-349-07'),\n",
    "        (6986658652, 'John Olson', 'John_Olson257@example.com', '@KX)FBXzP7Ue', '606.814.97'),\n",
    "        (5870824764, 'Gary Malone', 'Gary_Malone659@example.com', '$7_HlF6g', '001-902-76'),\n",
    "        (3463026079, 'Kristin Sanders', 'Kristin_Sanders125@example.com', 'k#5nOh!g', '+1-818-794'),\n",
    "        (7991496591, 'Martha Jacobs', 'Martha_Jacobs491@example.com', '3rgJcyE#*6M', '2462808562'),\n",
    "        (5822830658, 'Jessica Schmidt', 'Jessica_Schmidt236@example.com', '_Za1L^P5_D54K&j', '257-432-61'),\n",
    "        (5020007556, 'Monica Lopez', 'Monica_Lopez113@example.com', 'K7V!gqGj)', '001-574-57')]\n",
    "      df_actual = spark.createDataFrame(data =data,schema=my_schema)\n",
    "      df = rename_columns(users,column_mapping={'user_id':'id','name':'names'})\n",
    "      assert_df_equality(df_actual, df.limit(7))\n",
    "\n",
    "  # Data type casting test cases\n",
    "  def assertion_cast_type_columns(self):\n",
    "    expected_data = [\n",
    "    (1, '01-01-2022', '2022-02-15'),\n",
    "    (2, '30-03-2022', '2022-04-10'),\n",
    "    (3, '20-05-2022', '2022-06-05')]\n",
    "    my_schema = StructType([\n",
    "                            StructField('id', StringType(), True),\n",
    "                            StructField('date_column_1', StringType(), True),\n",
    "                            StructField('date_column_2', StringType(), True),\n",
    "                            ])\n",
    "\n",
    "    input_data = [\n",
    "        (1, '2022-01-01', '2022-02-15'),\n",
    "        (2, '2022-03-30', '2022-04-10'),\n",
    "        (3, '2022-05-20', '2022-06-05')]\n",
    "    df_date = spark.createDataFrame(input_data,my_schema)\n",
    "\n",
    "    df_expected = spark.createDataFrame(data=expected_data, schema=my_schema)\n",
    "    df_expected = df_expected.withColumn('date_column_1', to_date(df_expected['date_column_1'], 'dd-MM-yyyy'))\n",
    "    column_datatypes = {'date_column_1': DateType()}\n",
    "    df_actual = typecast_columns(df_date, column_datatypes)\n",
    "    assert_df_equality(df_actual, df_expected)\n",
    "\n",
    "  #Epoch Date conversion test cases\n",
    "  def assertion_convert_epoch_columns(self):\n",
    "    data = [(1,'1975-02-16 23:36:02'),\n",
    "                (2,'1975-02-16 23:36:12'),\n",
    "                (3,'1975-02-16 23:36:22')]\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"epoch_timestamp\", StringType(), True)])    \n",
    "    df_expected = spark.createDataFrame(data=data,schema=schema)\n",
    "    data = [\n",
    "    (1, 1618257621),  # Replace with your epoch timestamp values\n",
    "    (2, 1618257721),\n",
    "    (3, 1618257821)]\n",
    "    schema = [\"id\", \"epoch_timestamp\"]\n",
    " \n",
    "    df_epoch = spark.createDataFrame(data, schema=schema)\n",
    "    df_actual = convert_epoch_columns_to_datetime(df_epoch,['epoch_timestamp'])\n",
    "    df_actual = df_actual.withColumn('epoch_timestamp',col('epoch_timestamp').cast(StringType()))\n",
    "    assert_df_equality(df_actual, df_expected)\n",
    "  \n",
    "  def assertion_get_table_data(self):\n",
    "    my_schema = StructType([\n",
    "                          StructField('user_id', LongType(), True),\n",
    "                          StructField('email1', StringType(), True),\n",
    "                          StructField('password', StringType(), True),\n",
    "                          StructField('phone', StringType(), True),\n",
    "                          StructField('first_name', StringType(), True),\n",
    "                          StructField('last_name', StringType(), True),\n",
    "                          StructField('load_date', StringType(), True)])\n",
    "    data = [\n",
    "    (275905542, '732c11b3b3cacfa817e2bc64f605832d', '&E3nBxQDE_o&P5o', '(236)691-3','Brianna','Holmes','2023-12-29'),\n",
    "     (9059841656, 'be06d7c848dee6a36effc7cedb5201e0', '(OX1AZu9K)', '(637)769-0','Cameron','Nelson','2023-12-29')]\n",
    "    df_expected = spark.createDataFrame(data=data,schema=my_schema)\n",
    "    \n",
    "    df_actual = get_table_data('cue_box', 'users', filter_condition = None).limit(2)\n",
    "    \n",
    "    \n",
    "    assert_df_equality(df_actual, df_expected)\n",
    "\n",
    "  \n",
    "result = TestFixtureArbitraryFiles().execute_tests()\n",
    "print(result.to_string())\n",
    "is_job = dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "if is_job:\n",
    "  result.exit(dbutils)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3540776582759333,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "cue_box_tests",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
